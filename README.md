# ETL Pipeline for E-commerce Data

![status](https://img.shields.io/badge/status-work--in--progress-yellow)

üöß **Project Status:** Currently in development  
‚úÖ **Completed:** ETL phases implemented  
üîÑ **Next Steps:** Testing -> Automation -> Docker
üéØ **Target:** Fully functional ETL Automated pipeline with reporting by end of **May 2025**

## üìÑ Description

This project implements a simple ETL (Extract, Transform, Load) pipeline to process data from the public [DummyJSON](https://dummyjson.com/) API. The goal is to download data about users, products, and carts, transform it using Pandas, and load it into a relational SQL database. The project serves as a demonstration of basic ETL principles and working with tools like Python, Pandas, and SQLAlchemy for a portfolio.

## ‚öôÔ∏è Features

* **Extract:** Downloads data (users, products, carts) from the DummyJSON API with implemented retry logic for increased reliability. **Saves** raw data in JSON format.
* **Transform:** **Loads** raw data, uses the Pandas library for **cleaning**, **transformation**, and data preparation:
    * Selection of relevant columns.
    * Renaming columns (e.g., to snake_case).
    * Data type conversion (numbers, dates, strings).
    * Handling nested data (normalization of carts into `carts` and `cart_items` tables).
    * Processing product reviews list (extracting comments, calculating review count).
    * Duplicate removal.
* **Database Schema Definition (DDL):**
    * Explicit DDL scripts (`.sql` files) are provided for SQLite, PostgreSQL, and MSSQL to define the target database schema, including tables, columns, data types, primary keys, foreign keys (with `ON DELETE CASCADE`), `NOT NULL`, and `UNIQUE` constraints.
    * Schema `etl` is created and used for PostgreSQL and MSSQL.
* **Load:**
    * Applies the appropriate DDL script to the target database to create/recreate the schema and tables.
    * Loads the transformed Pandas DataFrames into the pre-defined SQL tables using SQLAlchemy and Pandas `to_sql()` method.
    * Supports SQLite, PostgreSQL, and MSSQL Server, selectable via configuration.
    * Handles MSSQL `IDENTITY_INSERT` appropriately for tables where IDs are provided from the source data versus generated by the database.

* **Logging:** Centralized logging system records pipeline progress to both the console (INFO level) and a rotating file (`logs/etl_pipeline.log`, DEBUG level) for monitoring and debugging.
* **Configuration:** Highly configurable via an `.env` file (for sensitive data and environment-specific settings like DB type and credentials) and a `config.py` file (for general settings like API endpoints, paths, and logging setup).
* **Code Quality & Workflow:** Utilizes tools like Black, isort, Pylint, dotenv-linter, Bandit, and a Makefile to ensure code quality, consistency, and streamline development (details in "Code Quality and Development Workflow" section).


## üóÇ Project Structure
```
data-pipeline-eshop/
‚îú‚îÄ‚îÄ .env.example              # Example environment file
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ data/                     # Stores raw JSON data and the SQLite DB file
‚îÇ   ‚îú‚îÄ‚îÄ carts_data.json
‚îÇ   ‚îú‚îÄ‚îÄ products_data.json
‚îÇ   ‚îú‚îÄ‚îÄ users_data.json
‚îÇ   ‚îî‚îÄ‚îÄ ecommerce_pipeline.db # SQLite database file (if used)
‚îú‚îÄ‚îÄ logs/                     # Stores pipeline log files
‚îÇ   ‚îî‚îÄ‚îÄ etl_pipeline.log
‚îú‚îÄ‚îÄ sql/                      # Contains DDL scripts for database schema creation
‚îÇ   ‚îú‚îÄ‚îÄ schema_mssql_ddl.sql
‚îÇ   ‚îú‚îÄ‚îÄ schema_postgresql_ddl.sql
‚îÇ   ‚îî‚îÄ‚îÄ schema_sqlite_ddl.sql
‚îú‚îÄ‚îÄ src/                      # Source code for pipeline modules
‚îÇ   ‚îú‚îÄ‚îÄ init.py
‚îÇ   ‚îú‚îÄ‚îÄ extract.py            # Module for data extraction from API
‚îÇ   ‚îú‚îÄ‚îÄ load.py               # Module for DDL application and loading data into DB
‚îÇ   ‚îú‚îÄ‚îÄ logging_setup.py      # Helper module for logging setup
‚îÇ   ‚îî‚îÄ‚îÄ transform.py          # Module for data transformation using Pandas
‚îú‚îÄ‚îÄ config.py                 # Main configuration file (loads .env)
‚îú‚îÄ‚îÄ main.py                   # Main script to run the ETL pipeline
‚îú‚îÄ‚îÄ Makefile                  # Makefile for common development tasks
‚îú‚îÄ‚îÄ poetry.lock
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ README.md                 # This file
```

## ‚öôÔ∏è Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/michalpuskac/etl-pipeline-for-ecommerce-data.git
    cd data-pipeline-eshop
    ```

2.  **Prerequisites:**
    * [Python 3.10+](https://www.python.org/downloads/) (or the version you are using)
    * [Poetry](https://python-poetry.org/) for dependency management
    * Docker (if running MSSQL or PostgreSQL in containers)
    * Necessary ODBC drivers if using MSSQL natively (e.g., Microsoft ODBC Driver 17 for SQL Server on macOS/Linux).

3.  **Create and configure `.env` file:**
    * Copy `.env.example` to `.env`: `cp .env.example .env`
    * Edit `.env` to set your `ETL_DB_TYPE` and the corresponding database credentials (see `.env.example` for details on variables like `PG_DB_USER`, `MSSQL_DB_PASSWORD`, etc.).

4.  **Install dependencies:**
    ```bash
    poetry install
    ```

    This will install all necessary libraries, including `pandas`, `requests`, `sqlalchemy`, `python-dotenv`, `psycopg2-binary` (for PostgreSQL), and `pyodbc` (for MSSQL).
5.  **Set up Pre-commit Hooks (Optional but Recommended):**
    ```bash
    pre-commit install
    ```

## Configuration

The primary configuration is managed through two files:

* **`.env` (in the project root):** Used for environment-specific settings and sensitive credentials. You **must** create this file from `.env.example` and fill in your details. Key variables:
    * `ETL_DB_TYPE`: "sqlite", "postgresql", or "mssql".
    * `SQLITE_DB_FILENAME`: Name of the SQLite database file.
    * `PG_DB_USER`, `PG_DB_PASSWORD`, `PG_DB_HOST`, `PG_DB_PORT`, `PG_DB_NAME`: Credentials for PostgreSQL.
    * `MSSQL_DB_USER`, `MSSQL_DB_PASSWORD`, `MSSQL_DB_HOST`, `MSSQL_DB_PORT`, `MSSQL_DB_NAME`, `MSSQL_DB_ODBC_DRIVER`: Credentials for MSSQL Server.
* **`config.py` (in the project root):**
    * Loads variables from `.env`.
    * Defines application-wide constants like `API_ENDPOINTS`, directory paths (`DATA_DIR`, `LOG_DIR`, `SQL_DIR`), `TARGET_DB_SCHEMA` ("etl"), and the `LOGGING_CONFIG` dictionary.
    * Dynamically constructs the `DB_CONNECTION_STRING` for SQLAlchemy based on `ETL_DB_TYPE`.


**Database Setup:**
* **SQLite:** The database file will be automatically created in the `data/` directory.
* **PostgreSQL/MSSQL:** You need to have a running server instance. The **database** specified by `PG_DB_NAME` or `MSSQL_DB_NAME` **must exist** on the server before running the pipeline. The pipeline will then create the `etl` schema (if it doesn't exist) and the tables within it.

## Usage

Run the complete ETL pipeline from the project root directory:

```bash
python main.py

The script will execute the Extract, Transform, and Load phases. Progress is logged to the console and to the logs/etl_pipeline.log file. The data will be loaded into an SQLite database file located at data/ecommerce_pipeline.db (by default).

The script will:

Set up the database schema by executing the appropriate DDL script from the sql/ directory.
Perform the Extract phase, downloading data to data/.
Perform the Transform phase, processing the data in memory.
Perform the Load phase, inserting transformed data into the target database.
Progress is logged to the console and to logs/etl_pipeline.log.

You can inspect the target database using tools like DB Browser for SQLite, pgAdmin (for PostgreSQL), or Azure Data Studio / SQL Server Management Studio (for MSSQL).
---

### üîç Code Quality and Development Workflow

This project employs a suite of tools and a `Makefile` to ensure high code quality, consistency, and an efficient development process. These practices help in maintaining a clean, readable, and robust codebase.

### Tools Used:

* **Black:** An uncompromising Python code formatter that automatically reformats code to a consistent style.
* **Pylint:** A static code analysis tool that checks for errors, enforces coding standards, identifies code smells, and offers suggestions for refactoring.
* **isort:** A Python utility to sort imports alphabetically and automatically separate them into sections and by type.
* **dotenv-linter:** A tool to validate `.env` files, helping to prevent common errors in environment variable definitions.
* **Bandit:** A tool designed to find common security issues in Python code.
* **Gitlint (via pre-commit hook):** Enforces conventional commit message styles, ensuring commit messages are descriptive and follow a consistent format. This is typically run automatically before each commit if pre-commit hooks are set up.
* **Pre-commit Hooks:** The project is set up to use pre-commit hooks (configuration in `.pre-commit-config.yaml`) to automatically run selected checks (like Black, isort, Pylint, Gitlint, etc.) before each commit. This helps catch issues early and maintain code standards.

### Makefile Commands:

A `Makefile` is provided to simplify common development tasks. Key commands include:

* `make format`: Formats all Python code in the current directory and subdirectories using **Black**.
* `make lint`: Runs **Pylint** on all Python files in the current directory and subdirectories to check for code quality issues and style violations.
* `make isort`: Sorts and formats import statements in Python files using **isort**.
* `make dotenv`: Validates the `.env` file using **dotenv-linter**.
* `make security`: Scans the `src` directory for common security vulnerabilities using **Bandit**.
* `make check`: A convenience command that runs `format`, `isort`, `lint`, `dotenv`, and `security` tasks sequentially, providing a comprehensive check of the codebase.

To use these commands, simply run them from the root directory of the project, for example:
```bash
make check
```


### üóÉÔ∏è Planned Features & Enhancements:**
The current implementation focuses on building a reliable and modular ETL pipeline. The following features and enhancements are planned to expand its capabilities and demonstrate more comprehensive end-to-end data engineering workflows:

-   ‚úÖ **Testing:**
    * Comprehensive unit tests (for individual functions) and integration tests (for pipeline segments)
-   üîÑ **Orchestration:**
    * Pipeline scheduling, monitoring, and dependency management using a workflow orchestration tool like Apache Airflow.
-   ‚ö†Ô∏è **Advanced Error Handling & Fault Tolerance:**
    * Improve pipeline-level error handling with more granular logging, notifications (e.g., email alerts on failure), and implement step-level error recovery or retry mechanisms.
-   üåê **Expand Data Sources & Transformations:**
    * Integrate additional APIs, files, or database sources to enrich the dataset.
    * Implement more complex data transformations, aggregations, or feature engineering based on potential analytical requirements.
-   üß™ **Secure Configuration Management:**
    * Manage sensitive configuration (especially database credentials for non-SQLite databases or production environments) securely using `.env` files and environment variables (e.g., using `python-dotenv`).
-   üí¨ **In-depth Text Analysis:**
    * Apply Natural Language Processing (NLP) techniques, such as sentiment analysis (e.g., using NLTK or TextBlob) and topic modeling, on the extracted product review comments for deeper customer insights.
-   üê≥ Containerization (Docker):
    * Containerize the entire ETL application using Docker for easier deployment and environment consistency.
    * Potentially use docker-compose to manage the pipeline application alongside database services for a complete development and testing environment.


## üìÑ License
This project is licensed under the MIT License. See the [LICENSE](https://github.com/michalpuskac/sql-data-warehouse-project/blob/main/LICENSE) file for details.


## üë®‚Äçüíª Author - Michal Pu≈°k√°ƒç
This project is part of my portfolio, showcasing skills and concepts I learned. If you have any questions, feedback, or would like to collaborate, feel free to get in touch!


<div align="left">
   <a href="https://www.linkedin.com/in/michal-pu%C5%A1k%C3%A1%C4%8D-94b925179/">
    <img src="https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn Badge"/>
  </a>
  <a href="https://github.com/michalpuskac">
    <img src="https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white" alt="GitHub Badge"/>
  </a>
</div>
